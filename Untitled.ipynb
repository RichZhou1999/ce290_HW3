{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89940bb7",
   "metadata": {},
   "source": [
    "# Task1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a556a0",
   "metadata": {},
   "source": [
    "## Reinforcement learning can be used in the field of health care"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36431a6e",
   "metadata": {},
   "source": [
    "Treatment for chronic diseases including cancer can be a long lasting process. \n",
    "Patients usually needs to have differnt kinds of treatment when they have various health conditions. \n",
    "Suppose that the treatment includes the usage of several appropriate doses of medication which can help release the pain or slow down the process of cancer.\n",
    "On the other hand, improper combination or dosage of drugs may worsen the patient's health condition.\n",
    "To form a Markov Decision Process. \n",
    "The state of this problem can be formulated as \n",
    "\n",
    "i) the concentration of tumor markers inside the blood ranging from 0 to 10 \n",
    "\n",
    "ii) the subjective uncomfortable feeling of the patient range from 0 to 10\n",
    "\n",
    "iii) the previous max usage of drug B\n",
    "\n",
    "The action space is the usage of two drugs\n",
    "\n",
    "\n",
    "drug A has the effect of decreasing the concentration of tumor markers while making the patinent uncomfortable, the usage of drug A starts from 0 to 9.\n",
    "\n",
    "drug B has the effect of making the patinents less painful, but after a few times of usage. It becomes useless. The usage of drug B starts from 0 to 9 and can only be added once someone starts to use.\n",
    "\n",
    "The assumption is that this kind of cancer will eventually lead the patient to the end of life. The final objective of the treatment is to extend the life of the patient while making the patient feel less painful. \n",
    "\n",
    "Given the final objective, if the patient can live one day more, there will be a constant reward of one. \n",
    "\n",
    "And there will also be some penalty due to the bad feeling of the patient.\n",
    "\n",
    "To more specfic, if nothing is done, the concentration of tumor markers will increase by 10% every day. If taken one dose of drug A, the speed of the rise of the concentration of tumor markers will decrease by 1%. Howerver, the side effect is that it will make the patient feel uncomfortable by 1%. Only by increasing the usage of drug B can decrease the rise of bad feelings. And once the usage of drug B is decreased or stopped, it will cause the increase of bad feelings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee566b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a839c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2358ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size_for_one = 10\n",
    "actions_prob = 1/action_size_for_one/action_size_for_one\n",
    "state_size_concentration = 91\n",
    "state_size_feeling = 91\n",
    "state_size_previous_usage_drug_B = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67c5b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 6], [6, 7], [6, 8], [6, 9], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 7], [7, 8], [7, 9], [8, 0], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7], [8, 8], [8, 9], [9, 0], [9, 1], [9, 2], [9, 3], [9, 4], [9, 5], [9, 6], [9, 7], [9, 8], [9, 9]]\n"
     ]
    }
   ],
   "source": [
    "intial_state = [1,1,0]\n",
    "action_space = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        action_space.append([i,j])\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7326217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_feeling_constant = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b72f8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    drugA_usage = action[0]\n",
    "    drugB_usage = action[1]\n",
    "    increase_rate_tumor = 10 - drugA_usage\n",
    "    increase_rate_bad_feeling = drugA_usage + state[2] - drugB_usage\n",
    "    \n",
    "    new_state = [0,0,0]\n",
    "    new_state[0] = np.round(state[0]*(1+increase_rate_tumor/100),1)\n",
    "    new_state[1] = np.round(state[1]*(1+increase_rate_bad_feeling/100),1)\n",
    "    new_state[0] = min(10, new_state[0])\n",
    "    new_state[1] = min(10, new_state[1])\n",
    "    new_state[0] = max(1, new_state[0])\n",
    "    new_state[1] = max(1, new_state[1])\n",
    "    new_state[2] = max(drugB_usage, state[2])\n",
    "    \n",
    "    reward = 1\n",
    "    reward -= bad_feeling_constant * new_state[1]\n",
    "    done = False\n",
    "    if ((new_state[1] >= 10) or (new_state[0] >= 10)):\n",
    "        done = True\n",
    "    return new_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ffcfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(state):\n",
    "    index = int((state - 1)/0.1)\n",
    "    if index == 91:\n",
    "        print(state)\n",
    "    return int((state - 1)/0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "470907c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value(max_iter=9, discount=1.0, policy = actions_prob*np.ones((state_size_concentration,state_size_feeling,state_size_previous_usage_drug_B,action_size_for_one*action_size_for_one))):\n",
    "    new_state_values = np.zeros((state_size_concentration,state_size_feeling,state_size_previous_usage_drug_B))\n",
    "    iteration = 0\n",
    "    \n",
    "    \n",
    "    while iteration<=max_iter:\n",
    "      state_values = new_state_values.copy()\n",
    "      old_state_values = state_values.copy()\n",
    "\n",
    "      for i in np.linspace(1,10,state_size_concentration):\n",
    "          for j in np.linspace(1,10,state_size_feeling):\n",
    "              for m in range(state_size_previous_usage_drug_B):\n",
    "                  i =  np.round(i,1)\n",
    "                  j =  np.round(j,1)\n",
    "                  index_i = get_index(i)\n",
    "                  index_j = get_index(j)\n",
    "                  value = 0\n",
    "                  for k,a in enumerate(actions):\n",
    "                      (next_i, next_j,next_m), reward, done = step([i, j, m], a)\n",
    "                      next_index_i = get_index(next_i)\n",
    "                      next_index_j = get_index(next_j)\n",
    "                      value += policy[index_i,index_j,m,k] * (reward + discount * state_values[next_index_i, next_index_j,next_m])\n",
    "                  new_state_values[index_i, index_j, m] = value\n",
    "\n",
    "      iteration += 1 \n",
    "      \n",
    "    return new_state_values, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d20ef7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.89518589  1.89051758  1.88414297 ...  1.82123657  1.80573438\n",
      "    1.78989006]\n",
      "  [ 1.78969354  1.7835197   1.77595129 ...  1.71220399  1.69641681\n",
      "    1.67926137]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-2.36466777 -2.37998596 -2.39308752 ... -2.42562194 -2.42719605\n",
      "   -2.42780821]\n",
      "  [-2.37908742 -2.39164364 -2.4019108  ... -2.4268671  -2.42780619\n",
      "   -2.42800821]\n",
      "  [-2.39077331 -2.40048975 -2.40846517 ... -2.42748137 -2.42800821\n",
      "   -2.42800821]]\n",
      "\n",
      " [[ 1.89518589  1.89051758  1.88414297 ...  1.82123657  1.80573438\n",
      "    1.78989006]\n",
      "  [ 1.78969354  1.7835197   1.77595129 ...  1.71220399  1.69641681\n",
      "    1.67926137]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-2.36466777 -2.37998596 -2.39308752 ... -2.42562194 -2.42719605\n",
      "   -2.42780821]\n",
      "  [-2.37908742 -2.39164364 -2.4019108  ... -2.4268671  -2.42780619\n",
      "   -2.42800821]\n",
      "  [-2.39077331 -2.40048975 -2.40846517 ... -2.42748137 -2.42800821\n",
      "   -2.42800821]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.89518589  1.89051758  1.88414297 ...  1.82123657  1.80573438\n",
      "    1.78989006]\n",
      "  [ 1.78969354  1.7835197   1.77595129 ...  1.71220399  1.69641681\n",
      "    1.67926137]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-2.36466777 -2.37998596 -2.39308752 ... -2.42562194 -2.42719605\n",
      "   -2.42780821]\n",
      "  [-2.37908742 -2.39164364 -2.4019108  ... -2.4268671  -2.42780619\n",
      "   -2.42800821]\n",
      "  [-2.39077331 -2.40048975 -2.40846517 ... -2.42748137 -2.42800821\n",
      "   -2.42800821]]\n",
      "\n",
      " [[ 1.89518589  1.89051758  1.88414297 ...  1.82123657  1.80573438\n",
      "    1.78989006]\n",
      "  [ 1.78969354  1.7835197   1.77595129 ...  1.71220399  1.69641681\n",
      "    1.67926137]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-2.36466777 -2.37998596 -2.39308752 ... -2.42562194 -2.42719605\n",
      "   -2.42780821]\n",
      "  [-2.37908742 -2.39164364 -2.4019108  ... -2.4268671  -2.42780619\n",
      "   -2.42800821]\n",
      "  [-2.39077331 -2.40048975 -2.40846517 ... -2.42748137 -2.42800821\n",
      "   -2.42800821]]\n",
      "\n",
      " [[ 1.89518589  1.89051758  1.88414297 ...  1.82123657  1.80573438\n",
      "    1.78989006]\n",
      "  [ 1.78969354  1.7835197   1.77595129 ...  1.71220399  1.69641681\n",
      "    1.67926137]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [-2.36466777 -2.37998596 -2.39308752 ... -2.42562194 -2.42719605\n",
      "   -2.42780821]\n",
      "  [-2.37908742 -2.39164364 -2.4019108  ... -2.4268671  -2.42780619\n",
      "   -2.42800821]\n",
      "  [-2.39077331 -2.40048975 -2.40846517 ... -2.42748137 -2.42800821\n",
      "   -2.42800821]]]\n"
     ]
    }
   ],
   "source": [
    "values, sync_iteration = compute_state_value(max_iter=3)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba5f02",
   "metadata": {},
   "source": [
    "## Reinforcement learning can be used in parking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074ca8d",
   "metadata": {},
   "source": [
    "We can teach the agent car to park itself.\n",
    "The state spaces include:\n",
    "\n",
    "i) The angle of the car\n",
    "\n",
    "ii) The velocity of the car\n",
    "\n",
    "iii) the position x of the car\n",
    "\n",
    "v) the position y of the car\n",
    "\n",
    "all the angles and positions are in global coordinate\n",
    "\n",
    "The objective of the agent is to park the car into position x_tar and y_tar within a error of err_x and err_y\n",
    "\n",
    "The reward function is:\n",
    "\n",
    "After one step, there will be a constant reward of -1.\n",
    "\n",
    "If the car crash or get into infeasible region, the reward is -100 and this episode ends.\n",
    "\n",
    "If the car get into the right place, the reward is 1000 this episode ends\n",
    "\n",
    "The action space:\n",
    "\n",
    "The agent can choose to accelerate or deaccelerate with the value ranging from -1 to 1 m/s^2.\n",
    "\n",
    "The agent can also choose to change the angle of the car with the angular aceeleration from -90 to 90 degree/s.\n",
    "\n",
    "Every step, the position of the car is updated based on the velocity and angle. The velocity is updated based on the acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83382ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
